"""
    commons functions used by all dags
    21.11.2024
"""
import json
import logging
import os
import random
import string
import sys
import pytz
from datetime import datetime, date, timedelta
from enum import Enum

import pendulum
import requests
from airflow import DAG
from airflow.exceptions import AirflowFailException, AirflowSkipException
from airflow.models import TaskInstance
from airflow.operators.bash import BashOperator
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator, BranchPythonOperator, ShortCircuitOperator
from airflow.sensors.python import PythonSensor
from airflow.sensors.time_sensor import TimeSensor
from airflow.utils.state import TaskInstanceState
from pendulum import DateTime, Duration
from typing import Dict, List, Tuple
from airflow.utils.task_group import TaskGroup

from dags.include.url_factory import url_get_json

Y_M_D = '%Y-%m-%d'
Y_M_D_H_M_S = '%Y-%m-%d %H:%M:%S'

logging.basicConfig()
logging.getLogger().setLevel(logging.ERROR)

IS_SECOND_RUN_PARAM = 'is_second_run'
GW_BASE_URL = "http://load-orchestrator-gw"
ULTIMO_BUSINESS_DATE_PATH = "calendar-ultimo-date"
REGULAR_BUSINESS_DATE_PATH = "calendar-regular-date"
WEEKLY_BUSINESS_DATE_PATH = "calendar-week-date"
FIRST_BUSINESS_DATE_PATH = "calendar-first-date"
PREVIOUS_BUSINESS_DATE_PATH = "calendar-previous-working-date"
GET_FIRST_WORKING_DATE_PATH = "calendar-get-first-working-date"
INQUIRY_DATE = 'inquiry_date'
IAM_FLOW = "IAM"
UNITY_FLOW = "UNITY"
WS3_FLOW = "WS3"
WS4_FLOW = "WS4"
DAY_ONE_LITERAL = "IS_DAY_ONE"
ENV = os.getenv("ENV")
TZ_BERLIN = 'Europe/Berlin'

# should match the texts that the splunk alert for failed dags and dags that missed sla is searching for
DAG_FAILED_MESSAGE = "DAG FAILED!"
TASK_MISSED_SLA_MESSAGE = "TASK MISSED SLA TIMEOUT!"

BUSINESS_PROCESSING_DATE_TEMPLATE = "{{ ti.xcom_pull(key='businessProcessingDate')}}"
FILTER_BY_BUSINESS_DATE_TEMPLATE = "date={{ ti.xcom_pull(key='businessProcessingDate') }}"

BASE_CALENDAR_DIRECT_URL = "http://tsp-calendar/tsp-calendar"
BATCH_OPERATOR_DIRECT_URL = "http://batch-operator/tsp-batch-operator"

# official taxonomy, taken from tables: TSP_PROCESSES, TSP_SECURITIZATION_TRANSACTION
TspProcess = Enum(value="TspProcess", names=["TSP_IMPORTER_PROCESS",
                                             "TSP_LOADER_PROCESS",
                                             "TXS_DAILY_EXPORTER_PROCESS",
                                             "TXS_MONTHLY_EXPORTER_PROCESS",
                                             "ABSM_DAILY_EXPORTER_PROCESS",
                                             "ABSM_MONTHLY_EXPORTER_PROCESS",
                                             "ABSM_DAILY_POPULATE_PROCESS"])

TspTransaction = Enum(value="TspTransaction", names=["MPF", "WS2", "WS3", "ALP", "DSL", "WS4"])

CobType = Enum(value="CobType", names=["DAILY", "MONTHLY"])

logging.basicConfig()
logging.getLogger().setLevel(logging.ERROR)


def _job_completed_sensor(
        previous_task_id: str,
        # job_id_name = job_id_CCO_PARTY_ADDRESS(_DELTA)?
        job_id_name: str,
        delta_step: bool = False,
        **context) -> bool:
    ti = context["ti"]
    job: int = int(ti.xcom_pull(task_ids=previous_task_id, key=job_id_name))
    if job == -1:
        logging.info("job completed skipped...")
        if delta_step:
            ti.xcom_push(key=f'CS_{job_id_name}', value="success")
        return True
    else:
        _url = f"{GW_BASE_URL}/api/getJobStatus/{job}"
        response = requests.request("GET", url=_url, verify=False)
        if response.status_code == 200:
            rsp = response.json()
            logging.info(f"### batch-operator response = {rsp}")
        else:
            if delta_step:
                ti.xcom_push(key=f'CS_{job_id_name}', value="failed")
            raise AirflowFailException(
                f"!!! batch-operator ({_url}) response text is {response.text}"
            )
        if response.json()["exitStatus"]["exitCode"] == "FAILED":
            if delta_step:
                ti.xcom_push(key=f'CS_{job_id_name}', value="failed")
            raise AirflowFailException(
                f"spring batch job {job} from task {previous_task_id} is in FAILED state!"
            )
        _rtr = response.json()["exitStatus"]["exitCode"] == "COMPLETED"
        if delta_step:
            ti.xcom_push(key=f'CS_{job_id_name}', value="success" if _rtr else "failed")
        return _rtr


def _is_ultimo_business_date(**context) -> bool:
    rsp: bool = _make_business_date(_business_date_path=ULTIMO_BUSINESS_DATE_PATH, **context)
    if not rsp:
        raise AirflowSkipException()
    return rsp


def _pick_monthly_branch(daily_branch, monthly_branch, **context):
    rsp: bool = _make_business_date(_business_date_path=ULTIMO_BUSINESS_DATE_PATH, **context)
    if not rsp:
        return daily_branch
    return monthly_branch


def _is_regular_business_date(**context) -> bool:
    ultimo_rsp: bool = _make_business_date(_business_date_path=ULTIMO_BUSINESS_DATE_PATH, **context)
    regular_rsp: bool = _make_business_date(_business_date_path=REGULAR_BUSINESS_DATE_PATH, **context)
    rsp: bool = regular_rsp and (not ultimo_rsp)
    if not rsp:
        raise AirflowSkipException()
    return rsp


def _is_ultimo_weekly_business_date(**context) -> bool:
    rsp: bool = _make_business_date(_business_date_path=WEEKLY_BUSINESS_DATE_PATH, **context)
    if not rsp:
        raise AirflowSkipException()
    return rsp


def _is_first_business_date(**context) -> bool:
    rsp: bool = _make_business_date(_business_date_path=FIRST_BUSINESS_DATE_PATH, **context)
    if not rsp:
        raise AirflowSkipException()
    return rsp


def _get_previous_working_date(**context) -> str:
    rsp: str = _get_previous_working_date_request(_previous_business_date_path=PREVIOUS_BUSINESS_DATE_PATH, **context)
    if not rsp:
        raise AirflowSkipException()
    return rsp


def get_first_working_date(**context) -> str:
    rsp: str = _get_first_working_date_request(_first_working_date_path=GET_FIRST_WORKING_DATE_PATH, **context)
    if not rsp:
        raise AirflowSkipException()
    return rsp


def get_business_date_from_ctx(**context) -> str:
    rsp: str = _get_previous_working_date_request(_previous_business_date_path=PREVIOUS_BUSINESS_DATE_PATH, **context)
    if not rsp:
        raise AirflowFailException("Unable to get business date from context")
    return rsp


def check_if_regular_business_date(iso_business_date: str) -> bool:
    ultimo = _check_if_business_day_of_type(ULTIMO_BUSINESS_DATE_PATH, iso_business_date)
    regular = _check_if_business_day_of_type(REGULAR_BUSINESS_DATE_PATH, iso_business_date)
    return regular and not ultimo


def check_continue_on_first_run(**context) -> bool:
    is_second_run = context["dag_run"].conf.get(IS_SECOND_RUN_PARAM, False)
    return not is_second_run


def check_if_ultimo_business_date(iso_business_date: str) -> bool:
    return _check_if_business_day_of_type(ULTIMO_BUSINESS_DATE_PATH, iso_business_date)


def _check_if_business_day_of_type(business_date_path: str, iso_business_date: str) -> bool:
    _url = f"{GW_BASE_URL}/api/{business_date_path}/{iso_business_date}"
    _response = url_get_json(_url, {})
    rsp: bool = bool(_response)
    logging.info(f"### IS {business_date_path}({iso_business_date} = {rsp}")
    return rsp


def _make_business_date(_business_date_path: str, **context) -> bool:
    # left_window = context["dag"].following_schedule(context["execution_date"])
    # right_window = context["dag"].following_schedule(left_window)
    # if not left_window < now <= right_window:
    #     raise AirflowSkipException()
    # now: DateTime = pendulum.now("UTC")
    # _month = "{:0>2}".format(now.month)
    # _day = "{:0>2}".format(now.day)
    # _businessDate = f"{now.year}-{_month}-{_day}"

    _iso_date, _basic_iso_date = get_business_date_as_iso(context)
    return _check_if_business_day_of_type(
        business_date_path=_business_date_path,
        iso_business_date=_iso_date
    )


def is_overwritten_execution(context) -> bool:
    return "NEXT_COB_DAY" in context["params"] and context["params"]["NEXT_COB_DAY"]


def get_request_date_from_context(context) -> str:
    utc_datetime = context['data_interval_end']
    local_datetime = utc_datetime.astimezone()  # converts to server time

    if is_overwritten_execution(context):
        # Get the desired date
        next_cob_day = datetime.strptime(context['params']['NEXT_COB_DAY'], "%Y-%m-%d").date()
        
        # Extract local time from the already-converted local_datetime
        local_time = local_datetime.time()
        
        # Combine new date with original local time
        local_datetime = datetime.combine(next_cob_day, local_time).astimezone()

    _request_date = local_datetime.strftime("%Y-%m-%d %H:%M:%S")
    print(f"### Running for server's local date and time REALLY FINAL???: {_request_date}")
    return _request_date


def date_to_str(dt: date, dt_fmt: str = Y_M_D) -> str:
    return dt.strftime(dt_fmt)


def str_to_date(dt_lit: str, dt_fmt: str = Y_M_D) -> date:
    return datetime.strptime(dt_lit, dt_fmt).date()


def first_date_of_month(dt: date) -> date:
    return dt.replace(day=1)


def _get_previous_working_date_request(_previous_business_date_path: str, **context) -> str:
    _request_date = get_request_date_from_context(context)
    modify_dag_run_conf(context, {'request_date': _request_date})
    _url = f"{GW_BASE_URL}/api/{_previous_business_date_path}/{_request_date}"
    _headers: dict[str, str] = {"Content-Type": "application/json"}
    _response = requests.request("GET", url=_url, headers=_headers, verify=False)
    if _response.status_code == 200:
        business_date: str = str(_response.json())
        bpd: str = business_date.replace("-", "")
        logging.info(f"### Business date for {_request_date} is {business_date}")
        logging.info(f"### Business date (BASIC_ISO): {bpd}")
        # TODO: `businessProcessingDate` to be deprecated in favour of `business_date`
        context["ti"].xcom_push(key="businessProcessingDate", value=bpd)
        # `business_date` in ISO format is meant to gradually replace `businessProcessingDate` in BASIC_ISO_FORMAT
        context["ti"].xcom_push(key="business_date", value=business_date)
        return bpd
    else:
        raise AirflowFailException(f"Exception calling {_url}")


def _get_first_working_date_request(_first_working_date_path: str, **context) -> str:
    _request_date = get_request_date_from_context(context)
    modify_dag_run_conf(context, {'request_date': _request_date})
    _url = f"{GW_BASE_URL}/api/{_first_working_date_path}/{_request_date}"
    _headers: dict[str, str] = {"Content-Type": "application/json"}
    _response = requests.request("GET", url=_url, headers=_headers, verify=False)
    if _response.status_code == 200:
        first_working_date: str = str(_response.json())
        logging.info(f"### First working date for {_request_date} is {first_working_date}")
        return first_working_date
    else:
        raise AirflowFailException(f"Exception calling {_url}")


def get_business_date_as_iso(context) -> (str, str):
    ti = context["ti"]
    _basic_iso_date: str = ti.xcom_pull(key='businessProcessingDate')
    if _basic_iso_date is None:
        _basic_iso_date = context["dag_run"].conf.get("businessProcessingDate")
    if _basic_iso_date is None:
        raise AirflowFailException("No `businessProcessingDate` in xcom, neither in Airflow context.")
    print(f"### _basic_iso_date = {_basic_iso_date}")
    _iso_date = _basic_iso_date[0:4] + "-" + _basic_iso_date[4:6] + "-" + _basic_iso_date[6:8]
    return _iso_date, _basic_iso_date


def _is_boutique_opened(**context) -> bool:
    inquiry_date = get_request_date_from_context(context)
    _url = f"{GW_BASE_URL}/api/calendar-regular-date/{inquiry_date}"
    _headers: dict[str, str] = {"Content-Type": "application/json"}
    _response = requests.request("GET", url=_url, headers=_headers, verify=False)
    if _response.status_code == 200:
        rsp: bool = bool(_response.json())
        logging.info(f"### boutique opened for({inquiry_date})={rsp}")
        if rsp:
            context["ti"].xcom_push(key=INQUIRY_DATE, value=inquiry_date)
        return rsp
    else:
        raise AirflowFailException(f"Exception calling {_url}")


def _is_ahead_of_eom(offset, **context) -> bool:
    inquiry_date = get_request_date_from_context(context)
    _url = f"{GW_BASE_URL}/api/calendar-ahead-of-EOM/{offset}/{inquiry_date}"
    _headers: dict[str, str] = {"Content-Type": "application/json"}
    _response = requests.request("GET", url=_url, headers=_headers, verify=False)
    if _response.status_code == 200:
        rsp: bool = bool(_response.json())
        logging.info(f"###{inquiry_date} is 5 days ahead of EOM ={rsp}")
        if rsp:
            context["ti"].xcom_push(key=INQUIRY_DATE, value=inquiry_date)
        return rsp
    else:
        raise AirflowFailException(f"Exception calling {_url}")


def ultimo_business_date_branch(task_id, daily_branch, monthly_branch) -> BranchPythonOperator:
    return BranchPythonOperator(
        task_id=task_id,
        python_callable=_pick_monthly_branch,
        op_kwargs={"daily_branch": daily_branch, "monthly_branch": monthly_branch}
    )


def ultimo_business_date_short_circuit() -> ShortCircuitOperator:
    return ShortCircuitOperator(
        task_id="calendar_ultimo_business_date_short_circuit",
        python_callable=_is_ultimo_business_date,
    )


def ahead_of_eom_short_circuit(offset) -> ShortCircuitOperator:
    return ShortCircuitOperator(
        task_id="ahead_of_eom_short_circuit",
        op_kwargs={'offset': offset},
        python_callable=_is_ahead_of_eom,
    )


def ultimo_business_date_filter() -> PythonOperator:
    return PythonOperator(
        task_id="calendar_ultimo_business_date",
        python_callable=_is_ultimo_business_date,
    )


def regular_business_date_filter() -> PythonOperator:
    return PythonOperator(
        task_id="calendar_regular_business_date",
        python_callable=_is_regular_business_date,
    )


def ultimo_weekly_business_date_filter() -> PythonOperator:
    return PythonOperator(
        task_id="calendar_ultimo_weekly_business_date",
        python_callable=_is_ultimo_weekly_business_date,
    )


def first_business_date_filter() -> PythonOperator:
    return PythonOperator(
        task_id="calendar_first_business_date",
        python_callable=_is_first_business_date,
    )


def wait_for_duration(sleep_duration: Duration, dag: DAG) -> BashOperator:
    task_id: str = "wait_" + "".join(random.choices(string.ascii_lowercase + string.digits, k=5))
    sleep_seconds = sleep_duration.total_seconds()
    return BashOperator(
        task_id=task_id,
        bash_command=f"sleep {sleep_seconds}",
        dag=dag,
    )


def get_previous_working_date() -> PythonOperator:
    return PythonOperator(
        task_id="get_business_date",
        python_callable=_get_previous_working_date,
    )


def boutique_opened() -> ShortCircuitOperator:
    return ShortCircuitOperator(
        task_id='in_every_business_day',
        python_callable=_is_boutique_opened,
    )


def _get_value_from_dict_fail_if_not_found(bag: dict, key: str) -> str:
    if bag is None or key not in bag or bag[key] is None:
        msg_abort = f"!!! `{bag}` not set or doesn't have `{key}` information. Mission aborted."
        logging.warning(msg_abort)
        raise AirflowFailException(msg_abort)
    return bag[key]


def is_ABSM_monthly_export_finished(tsp_transaction: TspTransaction) -> ShortCircuitOperator:
    kv = {
        "tsp_transaction": tsp_transaction
    }
    return ShortCircuitOperator(
        task_id='ABSM_monthly_export_finished',
        python_callable=_is_cob_for_ABSM_monthly_export_finished,
        op_kwargs=kv
    )


def is_ABSM_daily_export_not_started_today(tsp_transaction: TspTransaction) -> ShortCircuitOperator:
    kv = {
        "tsp_transaction": tsp_transaction
    }
    return ShortCircuitOperator(
        task_id='ABSM_daily_export_not_started_today',
        python_callable=_is_cob_for_ABSM_daily_export_not_started_today,
        op_kwargs=kv
    )


def _is_cob_for_ABSM_daily_export_not_started_today(tsp_transaction: TspTransaction, **context) -> bool:
    proc_status = get_process_status_started_today(
        process_name=TspProcess.ABSM_DAILY_EXPORTER_PROCESS,
        transaction_code=tsp_transaction)
    print(f"### process_status ABSM_DAILY_EXPORT for today={proc_status}")
    return proc_status is ""


def _is_cob_for_ABSM_monthly_export_finished(tsp_transaction: TspTransaction, **context) -> bool:
    request_date = context['dag_run'].conf['request_date']
    eom_date = get_eom_date(request_date)
    proc_status = get_daily_process_status(
        business_date=eom_date,
        process_name=TspProcess.ABSM_MONTHLY_EXPORTER_PROCESS,
        transaction_code=tsp_transaction)
    print(f"### process_status ABSM_MONTHLY(COB={eom_date})={proc_status}")
    return proc_status is "1"


def now_yyyy_mm_dd() -> str:
    now: DateTime = pendulum.now("UTC")
    _month = "{:0>2}".format(now.month)
    _day = "{:0>2}".format(now.day)
    return f"{now.year}-{_month}-{_day}"


def change_key_in_dict(dict_obj: dict, old_key, new_key) -> None:
    if old_key in dict_obj:
        dict_obj[new_key] = dict_obj.pop(old_key)
    else:
        print(f"Key '{old_key}' not found in dictionary.")


def get_eom_date(inquiry_date: str) -> str:
    endpoint = "/last-working-day-of-previous-month"
    path = f"/{inquiry_date}"
    url = (
        f"{BASE_CALENDAR_DIRECT_URL}"
        f"{endpoint}"
        f"{path}"
    )
    headers = {
        'Content-Type': 'application/json'
    }
    response = requests.request(method="GET", url=url, headers=headers, verify=False)
    logging.info(response)
    if response.status_code != 200:
        logging.exception(f"!!! Exception when calling {BASE_CALENDAR_DIRECT_URL}{endpoint}: {response.status_code}, "
                          f"{response.text}")
        raise AirflowFailException(f"Request to {endpoint} failed!")
    return response.json()


def get_daily_process_status(business_date: str, process_name: TspProcess, transaction_code: TspTransaction) -> str:
    return _get_daily_process_status(
        business_date=business_date,
        process_name=process_name.name,
        transaction_code=transaction_code.name
    )


def get_process_status_started_today(process_name: TspProcess, transaction_code: TspTransaction) -> str:
    return _get_process_status_started_today(
        process_name=process_name.name,
        transaction_code=transaction_code.name
    )


def _get_process_status_started_today(process_name: str, transaction_code: str) -> str:
    endpoint = "/get-process-status-started-today"
    query = f"?process_name={process_name}&transaction_code={transaction_code}"
    url = (
        f"{BATCH_OPERATOR_DIRECT_URL}"
        f"{endpoint}"
        f"{query}"
    )
    headers = {
        'Content-Type': 'application/json'
    }
    response = requests.request(method="GET", url=url, headers=headers, verify=False)
    logging.info(response)
    if response.status_code != 200:
        msg = f"!!! Exception when calling {BATCH_OPERATOR_DIRECT_URL}{endpoint}: {response.status_code} {response.text}"
        print(msg)
        raise AirflowFailException(f"Request to {endpoint} failed!")
    return response.text


def _get_daily_process_status(business_date: str, process_name: str, transaction_code: str) -> str:
    endpoint = "/get-daily-process-status"
    query = f"?business_date={business_date}&process_name={process_name}&transaction_code={transaction_code}"
    url = (
        f"{BATCH_OPERATOR_DIRECT_URL}"
        f"{endpoint}"
        f"{query}"
    )
    headers = {
        'Content-Type': 'application/json'
    }
    response = requests.request(method="GET", url=url, headers=headers, verify=False)
    logging.info(response)
    if response.status_code != 200:
        msg = f"!!! Exception when calling {BATCH_OPERATOR_DIRECT_URL}{endpoint}: {response.status_code} {response.text}"
        print(msg)
        raise AirflowFailException(f"Request to {endpoint} failed!")
    return response.text


def get_offset_and_inquiry_date(inquiry_date: str, offset: int) -> (date, date):
    eom_date_str: str = get_eom_date(inquiry_date=inquiry_date)
    eom_dt: date = str_to_date(eom_date_str)
    inquiry_dt: date = str_to_date(inquiry_date)
    offset_date_str: str = get_offset_date(inquiry_date=eom_date_str, offset=offset)
    offset_date: date = str_to_date(offset_date_str)
    print(f'### offset_date: {offset_date}, eom_dt: {eom_dt}, inquiry_dt: {inquiry_dt}')
    return offset_date, inquiry_dt


def get_offset_date(inquiry_date: str, offset: int) -> str:
    endpoint = "/offsetWorkingDate"
    path = f"/{offset}/{inquiry_date}"
    url = (
        f"{BASE_CALENDAR_DIRECT_URL}"
        f"{endpoint}"
        f"{path}"
    )
    headers = {
        'Content-Type': 'application/json'
    }
    response = requests.request(method="GET", url=url, headers=headers, verify=False)
    logging.info(response)
    if response.status_code != 200:
        logging.exception(f"!!! Exception when calling {BASE_CALENDAR_DIRECT_URL}{endpoint}: {response.status_code}, "
                          f"{response.text}")
        raise AirflowFailException(f"Request to {endpoint} failed!")
    return response.json()


def _save_daily_process_status(process_name: str, transaction_code: str,
                               process_status: int, **context) -> None:
    ti = context['ti']
    dag_run_xconf = ti.xcom_pull(key='dag_run_conf_xcom')
    if dag_run_xconf is None or 'business_date' not in dag_run_xconf or dag_run_xconf['business_date'] is None:
        msg_abort = f"!!! `dag_run_conf_xcom` not set or doesn't have `business_date` information. Mission aborted."
        logging.warning(msg_abort)
        logging.warning(f"!!! `dag_run_xconf` is {dag_run_xconf}")
        raise AirflowFailException(msg_abort)

    logging.info(f"### dag_run_xconf is {dag_run_xconf}")
    business_date = dag_run_xconf['business_date']
    logging.info(f"### Process {process_name}, tx {transaction_code}, business_date {business_date} to persist.")

    endpoint = "/save-daily-process"
    url = (
        f"{BATCH_OPERATOR_DIRECT_URL}"
        f"{endpoint}"
    )
    headers: dict[str, str] = {
        "Content-Type": "application/json"
    }
    body_dict = {
        'businessDate': f"{business_date}",
        'processName': f"{process_name}",
        'transactionCode': f"{transaction_code}",
        'processStatus': process_status
    }
    response = requests.request(
        method="POST",
        url=url,
        headers=headers,
        data=json.dumps(body_dict),
        verify=False)
    logging.info(response)
    if response.status_code != 200:
        logging.exception(f"!!! Exception when calling {BATCH_OPERATOR_DIRECT_URL}{endpoint}: {response.status_code}, "
                          f"{response.text}")
        raise AirflowFailException(f"Request to {endpoint} failed!")


def save_process_status_def(process: TspProcess, tsp_transaction: TspTransaction,
                            process_status: int, counter: int) -> PythonOperator:
    if process.name == TspProcess.ABSM_DAILY_EXPORTER_PROCESS:
        raise AirflowFailException(f"ABSM daily exporter should call specific method for saving process status.")
    task_id = f'Save_{process.name}_{tsp_transaction.name}_{process_status}_{counter}'
    kv = {
        "process_name": process.name,
        "transaction_code": tsp_transaction.name,
        "process_status": process_status
    }
    rtr = PythonOperator(
        task_id=task_id,
        python_callable=_save_daily_process_status,
        op_kwargs=kv
    )
    return rtr


def modify_dag_run_conf(context, new_dict: dict) -> None:
    cnf: dict = context['dag_run'].conf
    print(f"### dag_run.conf before image is {cnf}")
    cnf.update(new_dict)
    context['dag_run'].conf = cnf
    print(f"### dag_run.conf after image is {context['dag_run'].conf}")
    ti = context['ti']
    ti.xcom_push(key='dag_run_conf_xcom', value=cnf)
    logging.info(f"### dag_run_conf_xcom after image is {ti.xcom_pull(key='dag_run_conf_xcom')}")


# not used yet, meant to replace modify_dag_run_conf
def push_dag_run_conf_to_xcom(context, new_dict: dict) -> None:
    cnf: dict = context['dag_run'].conf.copy()
    logging.info(f"### dag_run.conf before image is {cnf}")
    cnf.update(new_dict)
    ti = context['ti']
    ti.xcom_push(key='dag_run_conf_xcom', value=cnf)
    logging.info(f"### dag_run_conf_xcom after image is {ti.xcom_pull(key='dag_run_conf_xcom')}")


def print_output_to_console(caller_component_id, output, run_id, dag_id, task_id):
    original_stdout = sys.stdout
    sys.stdout = sys.__stdout__
    current_time = datetime.now().strftime('%Y-%m-%dT%H:%M:%S.%f%z')
    message = f"[{current_time}] ### BEGIN {caller_component_id}: \nEnvironment: {ENV} Message: {output} Dag_id: {dag_id} Run_id: {run_id} Task_id: {task_id}\nEND {caller_component_id}".replace(
        "\n", f"\n[{current_time}] ### ")
    message += "\n"
    sys.stdout.write(message)
    sys.stdout = original_stdout


def print_output_to_console_dict(output: Dict[str, str], caller_component_id):
    original_stdout = sys.stdout
    sys.stdout = sys.__stdout__
    current_time = datetime.now().strftime('%Y-%m-%dT%H:%M:%S.%f%z')
    message = f"[{current_time}] ### BEGIN {caller_component_id}:\n[{current_time}]"
    for key in output.keys():
        message += f" [{key}: {output.get(key)}]"
    message += f"\n[{current_time}] ### END {caller_component_id}"
    message += "\n"
    sys.stdout.write(message)
    sys.stdout = original_stdout


def print_failed_dag_output(context):
    print_output_to_console(context['dag_run'].dag_id, DAG_FAILED_MESSAGE, context['dag_run'].run_id,
                            context['dag_run'].dag_id, context.get('task_instance').task_id)


def print_sla_missed_dag_output(dag_run, task_instance, expected_timestamp, desired_state, actual_stae, message):
    output: Dict[str, str] = {
        "Environment": ENV,
        "Message": f'{TASK_MISSED_SLA_MESSAGE} {message}',
        "Dag_id": dag_run.dag_id,
        "Dag_run_id": dag_run.run_id,
        "Task_id": task_instance.task_id,
        "Timestamp_expected_for_desired_state": expected_timestamp,
        "Desired_state": desired_state,
        "Actual_state": actual_stae
    }
    print_output_to_console_dict(output, dag_run.dag_id)


def make_time_sensor(id, expected_timestamp) -> TimeSensor:
    time_sensor: TimeSensor = TimeSensor(
        task_id=id,
        target_time=expected_timestamp
    )
    return time_sensor


def unroll_monitoring_tasks(task_group_name, task_param_list) -> TaskGroup:
    with TaskGroup(group_id=task_group_name) as tg:
        tasks = []
        for task_params in task_param_list:
            logging.info("params: " + str(task_params))
            tasks.append(make_check_task_state_sensor(task_params[0], task_params[1], task_params[2], task_params[3]))
        Begin = EmptyOperator(task_id="Begin")
        End = EmptyOperator(task_id="End", trigger_rule='all_done')
        Begin >> tasks >> End
    return tg


def make_check_task_state_sensor(monitored_task_id, desired_state: TaskInstanceState, message,
                                 expected_timestamp) -> PythonSensor:
    check_task_state_sensor: PythonSensor = PythonSensor(
        task_id=f'runtime_check_{monitored_task_id}_{desired_state.value}',
        python_callable=check_task_state,
        op_kwargs={'task_id': monitored_task_id, 'desired_state': desired_state.value,
                   'expected_timestamp': expected_timestamp, 'message': message},
        mode='reschedule'
    )
    return check_task_state_sensor


def check_task_state(task_id, desired_state, expected_timestamp, message, **context):
    task_instance: TaskInstance = get_task_instance_from_db_by_id(task_id, context)
    actual_state = task_instance.state
    if datetime.now().time() < expected_timestamp:
        if actual_state != desired_state:
            if actual_state is None or task_instance.end_date is None:
                logging.info(f"{task_id} not started or finished yet!")
                return False
            if actual_state == TaskInstanceState.SUCCESS.value:
                logging.info(f"{task_id} successful, stop monitoring!")
                return True
            print_sla_missed_dag_output(context['dag_run'], task_instance, expected_timestamp, desired_state, actual_state, message)
            return True
        logging.warning(f"{task_id} successful, stop monitoring!")
        return True
    logging.warning(f"{task_id} exceeded sla time!")
    print_sla_missed_dag_output(context['dag_run'], task_instance, expected_timestamp, desired_state, actual_state, message)
    return True


def get_task_instance_from_db_by_id(task_id, context) -> TaskInstance:
    dag: DAG = context['dag']
    execution_date = context['execution_date']
    task = dag.get_task(task_id)
    task_instance = TaskInstance(task=task, execution_date=execution_date)
    task_instance.refresh_from_db()
    return task_instance


'''
def build_monitoring_task(monitored_task_id, expected_execution_time: timedelta,
                          max_number_of_alerts=1) -> PythonSensor:
    runtime_check: PythonSensor = PythonSensor(
        task_id='runtime_check_' + monitored_task_id,
        python_callable=check_task_running,
        mode='reschedule',
        poke_interval=expected_execution_time,
        soft_fail=True,
        timeout=expected_execution_time * max_number_of_alerts,
        op_kwargs={'task_id': monitored_task_id, 'expected_execution_time': expected_execution_time}
    )
    return runtime_check


def check_task_running(task_id, expected_execution_time, **context):
    dag: DAG = context['dag']
    execution_date = context['execution_date']
    task = dag.get_task(task_id)
    task_instance = TaskInstance(task=task, execution_date=execution_date)
    task_instance.refresh_from_db()
    if task_instance.state == 'running':
        passed_time = datetime.now() - task_instance.start_date.replace(tzinfo=None)
        if passed_time > expected_execution_time:
            print_sla_missed_dag_output(context['dag_run'], task_instance, expected_execution_time, passed_time)
    else:
        if task_instance.state == 'failed':
            return False
        else:
            return True
'''
